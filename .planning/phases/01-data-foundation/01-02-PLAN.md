---
phase: 01-data-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/services/__init__.py
  - app/services/candle_ingestor.py
  - app/workers/jobs.py
  - app/workers/scheduler.py
  - app/api/candles.py
  - app/schemas/candle.py
autonomous: true
user_setup:
  - service: twelvedata
    why: "Market data API for XAUUSD OHLCV candles"
    env_vars:
      - name: TWELVE_DATA_API_KEY
        source: "Sign up at https://twelvedata.com, get API key from dashboard"

must_haves:
  truths:
    - "System fetches XAUUSD candles from Twelve Data for M15, H1, H4, D1 timeframes and stores them in PostgreSQL"
    - "Repeated fetches of the same time range do not create duplicate candles (upsert deduplication)"
    - "System only fetches candles newer than the latest stored timestamp (incremental fetch, not full re-download)"
    - "Gap detection identifies missing candles within a time range and logs them"
    - "APScheduler runs candle refresh jobs automatically at intervals aligned to candle close times"
    - "GET /candles/{timeframe} returns stored candles from the database"
  artifacts:
    - path: "app/services/candle_ingestor.py"
      provides: "Twelve Data client, fetch, upsert, gap detection"
      contains: "on_conflict_do_update"
    - path: "app/workers/jobs.py"
      provides: "Scheduled job functions for each timeframe"
      contains: "refresh_candles"
    - path: "app/workers/scheduler.py"
      provides: "APScheduler with registered candle refresh jobs"
      contains: "add_job"
    - path: "app/api/candles.py"
      provides: "GET /candles/{timeframe} endpoint"
      contains: "candles"
    - path: "app/schemas/candle.py"
      provides: "Pydantic response schema for candle data"
      contains: "CandleResponse"
  key_links:
    - from: "app/workers/jobs.py"
      to: "app/services/candle_ingestor.py"
      via: "Job calls ingestor.fetch_and_store()"
      pattern: "fetch_and_store|ingest"
    - from: "app/services/candle_ingestor.py"
      to: "app/models/candle.py"
      via: "PostgreSQL upsert using pg_insert"
      pattern: "pg_insert\\(Candle\\)"
    - from: "app/workers/scheduler.py"
      to: "app/workers/jobs.py"
      via: "scheduler.add_job registers refresh functions"
      pattern: "add_job.*refresh"
    - from: "app/api/candles.py"
      to: "app/database.py"
      via: "get_session dependency injection"
      pattern: "Depends\\(get_session\\)"
---

<objective>
Build the Twelve Data ingestion service that fetches XAUUSD OHLCV candles, upserts them into PostgreSQL with deduplication, detects gaps in the time series, and runs on an automatic schedule via APScheduler. Also add a candles API endpoint for querying stored data.

Purpose: This is the core data pipeline -- without reliable candle ingestion, no strategy, backtest, or signal can function. Aggressive caching and gap detection are critical given Twelve Data's 800 req/day free tier limit.

Output: A working ingestion pipeline that fetches candles from Twelve Data, stores them with deduplication, detects gaps, and refreshes on schedule. Plus a REST endpoint to query stored candles.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build Twelve Data candle ingestion service with upsert and gap detection</name>
  <files>
    app/services/__init__.py
    app/services/candle_ingestor.py
  </files>
  <action>
    Create the candle ingestion service that handles all Twelve Data interaction, PostgreSQL upsert, and gap detection:

    **app/services/__init__.py** -- Empty init file.

    **app/services/candle_ingestor.py** -- `CandleIngestor` class:

    **Interval mapping:**
    ```python
    INTERVAL_MAP = {"M15": "15min", "H1": "1h", "H4": "4h", "D1": "1day"}
    INTERVAL_TIMEDELTA = {"M15": timedelta(minutes=15), "H1": timedelta(hours=1), "H4": timedelta(hours=4), "D1": timedelta(days=1)}
    ```

    **Constructor:**
    - Accept `api_key: str`
    - Create `TDClient(apikey=api_key)`

    **`async def fetch_candles(self, symbol, timeframe, outputsize, start_date) -> list[dict]`:**
    - Map timeframe via `INTERVAL_MAP`
    - Call `self.client.time_series(symbol="XAU/USD", interval=..., outputsize=..., timezone="UTC", order="asc", start_date=...)`
    - Convert to pandas via `.as_pandas()`
    - Parse each row into dict with keys: symbol ("XAUUSD"), timeframe, timestamp (UTC-aware), open, high, low, close, volume
    - CRITICAL: Always pass `timezone="UTC"` to Twelve Data. Ensure timestamps have `tzinfo=timezone.utc`.
    - Wrap in `@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=30))` from tenacity
    - Log fetch count with loguru: `logger.info("Fetched {count} candles", symbol=..., timeframe=..., count=...)`
    - Handle empty responses gracefully (return empty list, log warning)

    **`async def upsert_candles(self, session: AsyncSession, candles: list[dict]) -> int`:**
    - Use `pg_insert(Candle).values(candles)` with `.on_conflict_do_update(index_elements=["symbol", "timeframe", "timestamp"], set_={open, high, low, close, volume})`
    - Commit and return count of rows affected
    - Log upsert count

    **`async def get_latest_timestamp(self, session: AsyncSession, symbol: str, timeframe: str) -> datetime | None`:**
    - Query `SELECT MAX(timestamp) FROM candles WHERE symbol=:symbol AND timeframe=:timeframe`
    - Return the latest stored timestamp (or None if no data exists)
    - Used to determine `start_date` for incremental fetches

    **`async def fetch_and_store(self, session: AsyncSession, symbol: str, timeframe: str, outputsize: int = 100) -> int`:**
    - Get latest timestamp via `get_latest_timestamp()`
    - If latest exists, compute `start_date = latest + 1 interval` to fetch only new candles
    - If no data exists, fetch `outputsize` candles (initial backfill)
    - Call `fetch_candles()` then `upsert_candles()`
    - Return count of candles stored
    - Log the full operation with bound context (symbol, timeframe, is_backfill, count)

    **`async def detect_gaps(self, session: AsyncSession, symbol: str, timeframe: str, start: datetime, end: datetime) -> list[datetime]`:**
    - Use PostgreSQL `generate_series()` LEFT JOIN anti-pattern from research:
      ```sql
      SELECT expected_ts FROM generate_series(:start, :end, :interval::interval) AS expected_ts
      LEFT JOIN candles c ON c.symbol = :symbol AND c.timeframe = :timeframe AND c.timestamp = expected_ts
      WHERE c.id IS NULL
      ORDER BY expected_ts
      ```
    - Map timeframe to interval string: M15->"15 minutes", H1->"1 hour", H4->"4 hours", D1->"1 day"
    - Filter out known non-trading periods:
      - Weekends: Saturday and Sunday (filter expected_ts where DOW is 0 or 6)
      - For now, use a simple weekend filter. Exact market break times can be refined in Plan 03.
    - Return list of missing timestamps
    - Log gap count at WARNING level if gaps found, INFO if no gaps

    IMPORTANT: All Twelve Data calls must pass `timezone="UTC"`. All timestamps must be timezone-aware. Use `Numeric` columns only (no float conversion for database storage). Wrap API calls with tenacity retry.
  </action>
  <verify>
    ```bash
    # Verify file exists with key patterns
    test -f app/services/candle_ingestor.py
    grep -q "on_conflict_do_update" app/services/candle_ingestor.py
    grep -q "generate_series" app/services/candle_ingestor.py
    grep -q "timezone.*UTC" app/services/candle_ingestor.py
    grep -q "retry" app/services/candle_ingestor.py
    grep -q "fetch_and_store" app/services/candle_ingestor.py
    ```
  </verify>
  <done>CandleIngestor class exists with fetch_candles (Twelve Data + retry), upsert_candles (pg_insert ON CONFLICT), get_latest_timestamp (incremental fetch), fetch_and_store (orchestrator), and detect_gaps (generate_series). All timestamps UTC. All API calls have retry logic.</done>
</task>

<task type="auto">
  <name>Task 2: Register APScheduler jobs and create candles API endpoint</name>
  <files>
    app/workers/jobs.py
    app/workers/scheduler.py
    app/api/candles.py
    app/schemas/candle.py
    app/main.py
  </files>
  <action>
    Wire up scheduled refresh jobs and a REST endpoint for querying candles:

    **app/schemas/candle.py** -- Pydantic response schemas:
    - `CandleResponse(BaseModel)` with fields: `id: int`, `symbol: str`, `timeframe: str`, `timestamp: datetime`, `open: float`, `high: float`, `low: float`, `close: float`, `volume: float | None`
    - Use `model_config = ConfigDict(from_attributes=True)` for ORM mode

    **app/workers/jobs.py** -- Scheduled job functions:
    - `async def refresh_candles(timeframe: str)`:
      - Create a new async session from `async_session_factory` (not via FastAPI Depends -- this runs outside request context)
      - Create a `CandleIngestor` instance with API key from settings
      - Call `ingestor.fetch_and_store(session, "XAUUSD", timeframe)`
      - After fetch, call `ingestor.detect_gaps(session, "XAUUSD", timeframe, start=7_days_ago, end=now)` to check recent gaps
      - Log completion with timeframe and candle count
      - Handle exceptions: log error with full traceback, do NOT let exception propagate (would kill the scheduler)
      - IMPORTANT: Use `try/except Exception` with `logger.exception()` to prevent job crashes

    **app/workers/scheduler.py** -- Register jobs (modify existing file from Plan 01):
    - After scheduler creation, add a `register_jobs()` function that registers 4 interval jobs:
      - M15: every 15 minutes, offset by 60 seconds (e.g., at :01, :16, :31, :46)
      - H1: every 60 minutes, offset by 60 seconds
      - H4: every 4 hours, offset by 60 seconds
      - D1: every 24 hours at 00:01 UTC
    - Use `CronTrigger` for precise alignment:
      - M15: `CronTrigger(minute="1,16,31,46", timezone="UTC")`
      - H1: `CronTrigger(minute=1, timezone="UTC")`
      - H4: `CronTrigger(hour="0,4,8,12,16,20", minute=1, timezone="UTC")`
      - D1: `CronTrigger(hour=0, minute=1, timezone="UTC")`
    - The 1-minute offset ensures the candle is fully closed before fetching (per research recommendation of 30-60s delay)
    - Call `register_jobs()` from the lifespan in `app/main.py` AFTER `scheduler.start()`

    **app/api/candles.py** -- Candles query router:
    - `router = APIRouter(prefix="/candles", tags=["candles"])`
    - `GET /candles/{timeframe}`:
      - Path param: `timeframe: str` (validate against "M15", "H1", "H4", "D1")
      - Query params: `limit: int = 100` (max 5000), `start: datetime | None = None`, `end: datetime | None = None`
      - Query candles from database ordered by timestamp DESC, applying filters
      - Return `list[CandleResponse]`
    - `GET /candles/{timeframe}/gaps`:
      - Returns detected gaps for a timeframe over the last 7 days
      - Calls `ingestor.detect_gaps()` and returns list of missing timestamps

    **app/main.py** -- Update to include candles router:
    - Import candles router from `app.api.candles`
    - `app.include_router(candles_router)`
    - In lifespan, after `scheduler.start()`, call `register_jobs()`

    IMPORTANT: Jobs run outside FastAPI request context -- create sessions directly from `async_session_factory`, not via `Depends(get_session)`. Always wrap job bodies in try/except to prevent scheduler crashes. Use CronTrigger (not IntervalTrigger) for precise candle-close alignment.
  </action>
  <verify>
    ```bash
    # Verify files exist
    test -f app/workers/jobs.py && test -f app/api/candles.py && test -f app/schemas/candle.py
    # Verify key patterns
    grep -q "CronTrigger" app/workers/scheduler.py
    grep -q "add_job" app/workers/scheduler.py
    grep -q "refresh_candles" app/workers/jobs.py
    grep -q "fetch_and_store" app/workers/jobs.py
    grep -q "CandleResponse" app/schemas/candle.py
    grep -q "candles" app/api/candles.py
    grep -q "register_jobs" app/main.py
    # Start app and verify endpoints exist
    timeout 10 uvicorn app.main:app --port 8766 &
    sleep 3
    curl -s http://localhost:8766/health | python3 -c "import sys,json; d=json.load(sys.stdin); print(d['status'])"
    curl -s http://localhost:8766/candles/H1?limit=5
    kill %1 2>/dev/null
    ```
  </verify>
  <done>APScheduler has 4 cron jobs (M15, H1, H4, D1) aligned to 1 minute after candle close. Jobs call CandleIngestor.fetch_and_store() with proper error handling. GET /candles/{timeframe} returns stored candles. GET /candles/{timeframe}/gaps returns detected gaps. App starts with all jobs registered.</done>
</task>

</tasks>

<verification>
Run these checks to confirm Plan 01-02 is complete:

1. App starts with 4 APScheduler jobs registered (check startup logs for job registration messages)
2. `curl http://localhost:8000/candles/H1?limit=5` returns candle data (after at least one fetch has run, or trigger manually)
3. `curl http://localhost:8000/candles/H1/gaps` returns gap analysis
4. Duplicate candle upsert: run fetch twice, verify candle count does not double
5. Incremental fetch: after initial backfill, subsequent fetches only request new candles (check logs for start_date parameter)
6. Error handling: temporarily set invalid API key, verify job logs error but scheduler continues running
</verification>

<success_criteria>
- CandleIngestor fetches from Twelve Data with timezone="UTC" and tenacity retry
- Upsert uses pg_insert ON CONFLICT DO UPDATE -- no duplicate candles possible
- Incremental fetch only requests candles newer than latest stored timestamp
- Gap detection uses generate_series() and filters weekends
- 4 cron jobs registered: M15 (:01,:16,:31,:46), H1 (:01), H4 (every 4h :01), D1 (00:01 UTC)
- GET /candles/{timeframe} returns stored candles with pagination
- All jobs wrapped in try/except to prevent scheduler crashes
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-02-SUMMARY.md`
</output>
