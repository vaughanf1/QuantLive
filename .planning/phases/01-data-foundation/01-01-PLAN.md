---
phase: 01-data-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - .env.example
  - .gitignore
  - app/__init__.py
  - app/main.py
  - app/config.py
  - app/database.py
  - app/models/__init__.py
  - app/models/base.py
  - app/models/candle.py
  - app/models/strategy.py
  - app/models/backtest_result.py
  - app/models/signal.py
  - app/models/outcome.py
  - app/models/strategy_performance.py
  - app/schemas/__init__.py
  - app/schemas/health.py
  - app/api/__init__.py
  - app/api/health.py
  - app/utils/__init__.py
  - app/utils/logging.py
  - app/workers/__init__.py
  - app/workers/scheduler.py
  - alembic.ini
  - alembic/env.py
  - alembic/versions/.gitkeep
autonomous: true

must_haves:
  truths:
    - "FastAPI application starts with `uvicorn app.main:app` and responds 200 on GET /health"
    - "PostgreSQL connection is established on startup and health endpoint reports database status"
    - "Alembic migration creates all tables (candles, strategies, backtest_results, signals, outcomes, strategy_performance)"
    - "Structured logging with loguru outputs formatted log lines on startup and request handling"
    - "All configuration is loaded from environment variables via pydantic-settings"
  artifacts:
    - path: "app/main.py"
      provides: "FastAPI application with lifespan, health router mounted"
      contains: "lifespan"
    - path: "app/config.py"
      provides: "Pydantic Settings class with all env vars"
      contains: "BaseSettings"
    - path: "app/database.py"
      provides: "Async engine, session factory, get_session dependency"
      contains: "async_sessionmaker"
    - path: "app/models/candle.py"
      provides: "Candle ORM model with NUMERIC precision and composite unique constraint"
      contains: "uq_candle_identity"
    - path: "app/api/health.py"
      provides: "GET /health endpoint with database connectivity check"
      contains: "health"
    - path: "app/utils/logging.py"
      provides: "Loguru configuration with intercept handler"
      contains: "InterceptHandler"
    - path: "alembic/env.py"
      provides: "Async Alembic env using async_engine_from_config"
      contains: "run_async_migrations"
  key_links:
    - from: "app/main.py"
      to: "app/database.py"
      via: "lifespan imports engine for dispose on shutdown"
      pattern: "engine\\.dispose"
    - from: "app/main.py"
      to: "app/workers/scheduler.py"
      via: "lifespan starts/stops scheduler"
      pattern: "scheduler\\.(start|shutdown)"
    - from: "app/api/health.py"
      to: "app/database.py"
      via: "get_session dependency injection"
      pattern: "Depends\\(get_session\\)"
    - from: "alembic/env.py"
      to: "app/models/base.py"
      via: "target_metadata import"
      pattern: "Base\\.metadata"
---

<objective>
Create the FastAPI application skeleton with PostgreSQL database, Alembic migrations, structured logging, and environment configuration. This is the foundation that every subsequent plan builds upon.

Purpose: Establish a working, runnable application with database connectivity so that Phase 1 Plans 02 and 03 can layer on data ingestion and validation without worrying about infrastructure.

Output: A FastAPI application that starts, connects to PostgreSQL, responds to /health, has all database tables created via Alembic, and logs structured output via loguru.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project foundation files (requirements, config, database, logging)</name>
  <files>
    requirements.txt
    .env.example
    .gitignore
    app/__init__.py
    app/config.py
    app/database.py
    app/utils/__init__.py
    app/utils/logging.py
  </files>
  <action>
    Create the project directory structure and foundation files:

    **requirements.txt** -- Pin all dependencies with minimum versions from research:
    ```
    fastapi>=0.115.0
    uvicorn[standard]>=0.32.0
    sqlalchemy[asyncio]>=2.0.36
    asyncpg>=0.30.0
    alembic>=1.14.0
    APScheduler>=3.10.4,<4.0
    pydantic>=2.10.0
    pydantic-settings>=2.7.0
    loguru>=0.7.3
    twelvedata[websocket]>=1.2.5
    httpx>=0.28.0
    tenacity>=9.0.0
    python-dotenv>=1.0.1
    pytest>=8.3.0
    pytest-asyncio>=0.24.0
    ```

    **.env.example** -- Template with all required env vars:
    ```
    DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/goldsignal
    TWELVE_DATA_API_KEY=your_api_key_here
    LOG_LEVEL=INFO
    LOG_JSON=false
    ```

    **.gitignore** -- Python gitignore including `.env`, `__pycache__`, `.venv`, `*.pyc`, `.pytest_cache`, `alembic/versions/*.py` (not .gitkeep).

    **app/__init__.py** -- Empty init file.

    **app/config.py** -- Pydantic Settings class:
    - `database_url: str` (required)
    - `twelve_data_api_key: str` (required)
    - `log_level: str = "INFO"`
    - `log_json: bool = False`
    - `candle_refresh_delay_seconds: int = 60` (delay after candle close before fetching)
    - Use `SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")`
    - Export `get_settings()` with `@lru_cache` for singleton pattern

    **app/database.py** -- Async database setup:
    - Import `Settings` via `get_settings()`
    - `create_async_engine()` with `pool_size=5`, `max_overflow=10`, `pool_pre_ping=True`, `pool_recycle=300`
    - `async_sessionmaker(engine, expire_on_commit=False)` stored as `async_session_factory`
    - `async def get_session() -> AsyncGenerator[AsyncSession, None]` as FastAPI dependency using `yield`
    - Export `engine` and `async_session_factory` for use in lifespan and tests

    **app/utils/logging.py** -- Loguru setup following research Pattern 6:
    - `setup_logging(log_level, json_output)` function
    - Remove default loguru handler, add stderr handler with format or serialize=True
    - `InterceptHandler` class to capture stdlib logging (uvicorn, sqlalchemy, alembic)
    - Configure `logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)`

    IMPORTANT: Do NOT use `@app.on_event("startup")` -- use lifespan. Do NOT create sync sessions. Use `Numeric(10, 2)` for prices, never FLOAT.
  </action>
  <verify>
    ```bash
    # Verify all files exist
    test -f requirements.txt && test -f .env.example && test -f .gitignore
    test -f app/__init__.py && test -f app/config.py && test -f app/database.py
    test -f app/utils/__init__.py && test -f app/utils/logging.py
    # Verify key patterns
    grep -q "BaseSettings" app/config.py
    grep -q "async_sessionmaker" app/database.py
    grep -q "InterceptHandler" app/utils/logging.py
    ```
  </verify>
  <done>All foundation files exist. Config uses pydantic-settings. Database module provides async engine with get_session dependency. Logging uses loguru with InterceptHandler.</done>
</task>

<task type="auto">
  <name>Task 2: Create all SQLAlchemy ORM models and Alembic migration setup</name>
  <files>
    app/models/__init__.py
    app/models/base.py
    app/models/candle.py
    app/models/strategy.py
    app/models/backtest_result.py
    app/models/signal.py
    app/models/outcome.py
    app/models/strategy_performance.py
    alembic.ini
    alembic/env.py
    alembic/versions/.gitkeep
  </files>
  <action>
    Create all ORM models and Alembic async migration infrastructure:

    **app/models/base.py** -- Shared DeclarativeBase:
    ```python
    from sqlalchemy.orm import DeclarativeBase

    class Base(DeclarativeBase):
        pass
    ```

    **app/models/candle.py** -- Full Candle model (from research):
    - Fields: `id` (BigInteger PK auto), `symbol` (String(10), default "XAUUSD"), `timeframe` (String(5)), `timestamp` (DateTime(timezone=True)), `open/high/low/close` (Numeric(10,2)), `volume` (Numeric(15,2) nullable)
    - UniqueConstraint on (symbol, timeframe, timestamp) named `uq_candle_identity`
    - Index on (symbol, timeframe, timestamp) named `idx_candles_lookup`
    - Use SQLAlchemy 2.0 `Mapped` and `mapped_column` style

    **Stub models** (for INFRA-02 -- tables needed now, logic added in later phases):

    **app/models/strategy.py** -- Strategy model:
    - `id` (BigInteger PK), `name` (String(50) unique), `description` (Text nullable), `is_active` (Boolean default True), `created_at` (DateTime(timezone=True) server_default=func.now()), `updated_at` (DateTime(timezone=True) onupdate=func.now())

    **app/models/backtest_result.py** -- BacktestResult model:
    - `id` (BigInteger PK), `strategy_id` (BigInteger ForeignKey strategies.id), `timeframe` (String(5)), `window_days` (Integer), `start_date`/`end_date` (DateTime(timezone=True)), `win_rate`/`profit_factor`/`sharpe_ratio`/`max_drawdown`/`expectancy` (Numeric(10,4) nullable), `total_trades` (Integer), `created_at` (DateTime(timezone=True))

    **app/models/signal.py** -- Signal model:
    - `id` (BigInteger PK), `strategy_id` (BigInteger FK), `symbol` (String(10)), `timeframe` (String(5)), `direction` (String(5), "BUY"/"SELL"), `entry_price`/`stop_loss`/`take_profit_1`/`take_profit_2` (Numeric(10,2)), `risk_reward` (Numeric(5,2)), `confidence` (Numeric(5,2)), `reasoning` (Text), `status` (String(20) default "active"), `created_at`/`expires_at` (DateTime(timezone=True))

    **app/models/outcome.py** -- Outcome model:
    - `id` (BigInteger PK), `signal_id` (BigInteger FK signals.id unique), `result` (String(20) -- "tp1_hit", "tp2_hit", "sl_hit", "expired"), `exit_price` (Numeric(10,2)), `pnl_pips` (Numeric(10,2)), `duration_minutes` (Integer), `created_at` (DateTime(timezone=True))

    **app/models/strategy_performance.py** -- StrategyPerformance model:
    - `id` (BigInteger PK), `strategy_id` (BigInteger FK), `period` (String(10) e.g. "7d", "30d"), `win_rate`/`profit_factor`/`avg_rr` (Numeric(10,4)), `total_signals` (Integer), `is_degraded` (Boolean default False), `calculated_at` (DateTime(timezone=True))

    **app/models/__init__.py** -- Import all models so Alembic autogenerate sees them:
    ```python
    from app.models.base import Base
    from app.models.candle import Candle
    from app.models.strategy import Strategy
    from app.models.backtest_result import BacktestResult
    from app.models.signal import Signal
    from app.models.outcome import Outcome
    from app.models.strategy_performance import StrategyPerformance
    ```

    **Alembic setup:**
    - Run `alembic init -t async alembic` to create async template (or manually create the files)
    - **alembic.ini**: Set `sqlalchemy.url` to empty string (will be overridden in env.py from app config)
    - **alembic/env.py**: Follow the async pattern from research exactly:
      - Import `Base` from `app.models` (this triggers all model imports)
      - Set `target_metadata = Base.metadata`
      - Override `sqlalchemy.url` from `app.config.get_settings().database_url` in `run_async_migrations()`
      - Use `async_engine_from_config` with `poolclass=pool.NullPool`
    - Create `alembic/versions/.gitkeep` so the directory is tracked

    After creating all files, generate the initial migration:
    ```bash
    alembic revision --autogenerate -m "initial schema - all tables"
    ```

    IMPORTANT: All DateTime columns must use `DateTime(timezone=True)`. All price columns use `Numeric(10, 2)`. All metric columns use `Numeric(10, 4)`. Never use Float for financial data.
  </action>
  <verify>
    ```bash
    # Verify all model files exist
    test -f app/models/base.py && test -f app/models/candle.py && test -f app/models/strategy.py
    test -f app/models/backtest_result.py && test -f app/models/signal.py
    test -f app/models/outcome.py && test -f app/models/strategy_performance.py
    # Verify Alembic setup
    test -f alembic.ini && test -f alembic/env.py
    # Verify Candle model has correct constraint names
    grep -q "uq_candle_identity" app/models/candle.py
    grep -q "idx_candles_lookup" app/models/candle.py
    # Verify models/__init__.py imports all models
    grep -q "Candle" app/models/__init__.py
    grep -q "Strategy" app/models/__init__.py
    grep -q "Signal" app/models/__init__.py
    # Verify migration file was generated
    ls alembic/versions/*.py | head -1
    # Run migration against database (requires running PostgreSQL)
    alembic upgrade head
    ```
  </verify>
  <done>All 6 ORM models exist with correct types (Numeric, DateTime(timezone=True)). Alembic async env.py imports all models. Initial migration is generated. `alembic upgrade head` creates all tables successfully.</done>
</task>

<task type="auto">
  <name>Task 3: Create FastAPI app with lifespan, health endpoint, and scheduler shell</name>
  <files>
    app/main.py
    app/schemas/__init__.py
    app/schemas/health.py
    app/api/__init__.py
    app/api/health.py
    app/workers/__init__.py
    app/workers/scheduler.py
  </files>
  <action>
    Wire together the FastAPI application with lifespan, health check, and APScheduler:

    **app/schemas/health.py** -- Pydantic response models:
    - `HealthResponse(BaseModel)` with fields: `status: str`, `database: str`, `timestamp: datetime`, `version: str = "0.1.0"`

    **app/api/health.py** -- Health check router:
    - `router = APIRouter(tags=["health"])`
    - `GET /health` endpoint:
      - Accepts `session: AsyncSession = Depends(get_session)`
      - Runs `SELECT 1` via `session.execute(text("SELECT 1"))` to verify database connectivity
      - Returns `HealthResponse(status="ok", database="connected", timestamp=datetime.now(UTC))`
      - On database error: returns `HealthResponse(status="degraded", database="disconnected", ...)` with 503 status
      - Log health check result with loguru

    **app/workers/scheduler.py** -- APScheduler setup:
    - Create `AsyncIOScheduler` with:
      - `MemoryJobStore` (no sync driver needed for Phase 1 -- per research recommendation)
      - `job_defaults={"coalesce": True, "misfire_grace_time": 300, "max_instances": 1}`
      - `timezone="UTC"`
    - Export `scheduler` instance (jobs will be registered in Plan 01-02)

    **app/main.py** -- Application entry point:
    - Import `setup_logging` from `app.utils.logging`
    - Import `get_settings` from `app.config`
    - Import `engine` from `app.database`
    - Import `scheduler` from `app.workers.scheduler`
    - Import `router` from `app.api.health`
    - `@asynccontextmanager async def lifespan(app: FastAPI)`:
      - Call `setup_logging(settings.log_level, settings.log_json)`
      - Start scheduler: `scheduler.start()`
      - Log "GoldSignal application started"
      - `yield`
      - Shutdown scheduler: `scheduler.shutdown(wait=False)`
      - Dispose engine: `await engine.dispose()`
      - Log "GoldSignal application stopped"
    - Create `app = FastAPI(title="GoldSignal", version="0.1.0", lifespan=lifespan)`
    - Include health router: `app.include_router(router)`

    **Init files** for schemas, api, workers packages -- empty `__init__.py` files.

    After creating all files, verify the app starts:
    ```bash
    # Install dependencies first
    pip install -r requirements.txt
    # Start the app (requires PostgreSQL running and .env configured)
    uvicorn app.main:app --host 0.0.0.0 --port 8000
    # In another terminal:
    curl http://localhost:8000/health
    ```

    IMPORTANT: Use `lifespan` asynccontextmanager, NOT `@app.on_event`. Use `MemoryJobStore` for APScheduler (not SQLAlchemyJobStore -- that requires a sync driver). Call `setup_logging()` early in lifespan so all startup logs are formatted.
  </action>
  <verify>
    ```bash
    # Verify files exist
    test -f app/main.py && test -f app/api/health.py && test -f app/workers/scheduler.py
    # Verify key patterns
    grep -q "lifespan" app/main.py
    grep -q "asynccontextmanager" app/main.py
    grep -q "scheduler.start" app/main.py
    grep -q "engine.dispose" app/main.py
    grep -q "GET" app/api/health.py || grep -q "get" app/api/health.py
    grep -q "MemoryJobStore\|AsyncIOScheduler" app/workers/scheduler.py
    # Integration test: start app, hit health endpoint
    # (requires PostgreSQL running with correct DATABASE_URL in .env)
    timeout 10 uvicorn app.main:app --port 8765 &
    sleep 3
    curl -s http://localhost:8765/health | python3 -c "import sys,json; d=json.load(sys.stdin); assert d['status']=='ok', f'Health failed: {d}'; print('Health OK')"
    kill %1 2>/dev/null
    ```
  </verify>
  <done>FastAPI app starts via `uvicorn app.main:app`. GET /health returns 200 with `{"status": "ok", "database": "connected", ...}`. Scheduler starts on lifespan. Engine disposes on shutdown. All logs flow through loguru.</done>
</task>

</tasks>

<verification>
Run these checks to confirm Plan 01-01 is complete:

1. `pip install -r requirements.txt` succeeds without errors
2. `alembic upgrade head` creates all 6 tables in PostgreSQL
3. `uvicorn app.main:app --port 8000` starts without errors
4. `curl http://localhost:8000/health` returns `{"status": "ok", "database": "connected", ...}`
5. Startup logs show loguru-formatted output (timestamps, levels, module names)
6. `alembic downgrade base && alembic upgrade head` (migration is reversible)
</verification>

<success_criteria>
- FastAPI application starts and responds 200 on GET /health with database status
- All 6 tables exist in PostgreSQL via Alembic migration (candles, strategies, backtest_results, signals, outcomes, strategy_performance)
- Candle model uses NUMERIC(10,2) for prices and DateTime(timezone=True) for timestamps
- APScheduler AsyncIOScheduler is running (no jobs registered yet -- that's Plan 02)
- All configuration loaded from .env via pydantic-settings
- Loguru captures all application and library logging
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-01-SUMMARY.md`
</output>
