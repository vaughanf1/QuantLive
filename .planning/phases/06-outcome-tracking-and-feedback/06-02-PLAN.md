---
phase: 06-outcome-tracking-and-feedback
plan: 02
type: tdd
wave: 2
depends_on: ["06-01"]
files_modified:
  - app/services/performance_tracker.py
  - tests/test_performance_tracker.py
  - app/services/strategy_selector.py
  - app/services/outcome_detector.py
autonomous: true

must_haves:
  truths:
    - "After each outcome, rolling 7d and 30d performance metrics are recalculated for that strategy"
    - "StrategyPerformance rows are upserted (not duplicated) per strategy+period combination"
    - "Win rate, profit factor, and avg R:R are computed from live signal outcomes (not backtest results)"
    - "StrategySelector incorporates live StrategyPerformance metrics into its scoring when available"
    - "Performance recalculation only triggers on NEW outcomes, not on every 30-second check"
  artifacts:
    - path: "app/services/performance_tracker.py"
      provides: "PerformanceTracker service class"
      exports: ["PerformanceTracker"]
    - path: "tests/test_performance_tracker.py"
      provides: "Unit tests for performance tracking"
      min_lines: 100
  key_links:
    - from: "app/services/performance_tracker.py"
      to: "app/models/outcome.py"
      via: "Query outcomes within rolling window for metric calculation"
      pattern: "Outcome"
    - from: "app/services/performance_tracker.py"
      to: "app/models/strategy_performance.py"
      via: "Upsert StrategyPerformance rows"
      pattern: "StrategyPerformance"
    - from: "app/services/outcome_detector.py"
      to: "app/services/performance_tracker.py"
      via: "Trigger recalculation after new outcome"
      pattern: "PerformanceTracker"
    - from: "app/services/strategy_selector.py"
      to: "app/models/strategy_performance.py"
      via: "Query latest StrategyPerformance for live metric integration"
      pattern: "StrategyPerformance"
---

<objective>
Build the PerformanceTracker service that recalculates rolling strategy performance metrics (7d, 30d) after each trade outcome, and integrate live metrics into strategy selection scoring.

Purpose: Without live performance tracking, the strategy selector relies only on backtest results. This plan closes the loop so that actual signal performance directly influences which strategy is selected next. A strategy that looks good on paper but fails live will be deprioritized.

Output: `PerformanceTracker` service class with tests, wired into OutcomeDetector for automatic recalculation, and StrategySelector updated to use live performance data.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-outcome-tracking-and-feedback/06-RESEARCH.md
@.planning/phases/06-outcome-tracking-and-feedback/06-01-SUMMARY.md

# Key existing code:
@app/models/outcome.py (Outcome model -- result, pnl_pips, signal_id, created_at)
@app/models/signal.py (Signal model -- strategy_id, risk_reward, direction)
@app/models/strategy_performance.py (StrategyPerformance -- win_rate, profit_factor, avg_rr, total_signals, is_degraded, period)
@app/services/strategy_selector.py (StrategySelector -- modify to incorporate live metrics)
@app/services/outcome_detector.py (OutcomeDetector -- wire PerformanceTracker call after outcome)
@app/services/metrics_calculator.py (MetricsCalculator -- reference for metric computation patterns)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PerformanceTracker service with TDD</name>
  <files>app/services/performance_tracker.py, tests/test_performance_tracker.py</files>
  <action>
**RED phase -- Write failing tests first** in `tests/test_performance_tracker.py`:

Test cases:
1. `test_recalculate_7d_win_rate` -- 3 wins + 2 losses in last 7 days -> win_rate = 0.6000
2. `test_recalculate_30d_profit_factor` -- Sum of winning pnl / abs(sum of losing pnl) -> correct profit_factor
3. `test_recalculate_avg_rr` -- Average of risk_reward values from associated signals -> correct avg_rr
4. `test_upsert_existing_row` -- StrategyPerformance row exists for same strategy+period -> updated (not duplicated)
5. `test_insert_new_row` -- No existing StrategyPerformance row -> new row created
6. `test_no_outcomes_in_window` -- Zero outcomes in the rolling window -> total_signals=0, win_rate=0, profit_factor=0
7. `test_both_periods_calculated` -- recalculate_for_strategy produces rows for both "7d" and "30d"
8. `test_only_relevant_strategy` -- Outcomes from other strategies are NOT included in the calculation
9. `test_profit_factor_no_losses` -- All wins (no losses) -> profit_factor capped at 9999.9999 (Numeric(10,4) compat)
10. `test_win_rate_counts_tp_hits_as_wins` -- tp1_hit and tp2_hit both count as wins; sl_hit and expired count as losses

Use mock signal+outcome data inserted into DB for tests that need it. For pure computation tests, pass data directly.

**GREEN phase -- Implement** `app/services/performance_tracker.py`:

```python
class PerformanceTracker:
    """Recalculates rolling performance metrics per strategy after outcomes.

    Computes win_rate, profit_factor, and avg_rr for 7d and 30d rolling
    windows. Results are upserted into the strategy_performance table.

    Win = tp1_hit or tp2_hit
    Loss = sl_hit or expired
    """

    PERIODS = {"7d": 7, "30d": 30}
    WIN_RESULTS = {"tp1_hit", "tp2_hit"}
    LOSS_RESULTS = {"sl_hit", "expired"}
    MAX_PROFIT_FACTOR = Decimal("9999.9999")  # Numeric(10,4) cap

    async def recalculate_for_strategy(
        self, session: AsyncSession, strategy_id: int
    ) -> list[StrategyPerformance]:
        """Recalculate 7d and 30d rolling metrics for one strategy.

        Queries outcomes+signals for this strategy within each period,
        computes metrics, and upserts StrategyPerformance rows.

        Returns list of upserted StrategyPerformance objects.
        """

    async def _compute_metrics(
        self, session: AsyncSession, strategy_id: int, period_label: str, days: int
    ) -> dict:
        """Query outcomes within rolling window and compute metrics.

        Returns dict with: win_rate, profit_factor, avg_rr, total_signals
        """
        # Query: JOIN Outcome + Signal WHERE signal.strategy_id = X
        #   AND outcome.created_at >= now() - timedelta(days=days)
        # Compute:
        #   - total_signals = count
        #   - wins = count where result in WIN_RESULTS
        #   - win_rate = wins / total_signals (0 if none)
        #   - gross_profit = sum(pnl_pips) where pnl_pips > 0
        #   - gross_loss = abs(sum(pnl_pips)) where pnl_pips < 0
        #   - profit_factor = gross_profit / gross_loss (capped at 9999.9999, 0 if no loss)
        #   - avg_rr = mean of signal.risk_reward for signals with outcomes

    async def _upsert_performance(
        self, session: AsyncSession, strategy_id: int, period: str, metrics: dict
    ) -> StrategyPerformance:
        """Upsert StrategyPerformance row for strategy+period.

        Query existing row by strategy_id + period (most recent).
        If exists and was calculated today, update it.
        Otherwise, create new row.
        """
```

Key implementation details:
- Profit factor: gross_profit / gross_loss. Cap at 9999.9999 for DB compatibility (decision [03-01]).
- If gross_loss == 0 (all wins), set profit_factor to MAX_PROFIT_FACTOR.
- If total_signals == 0, all metrics = 0.
- Win rate: Decimal(str(round(wins/total, 4))).
- Use `Outcome.created_at` as the timestamp for windowing (when the outcome happened).
- `is_degraded` field: leave False in PerformanceTracker. FeedbackController (plan 06-03) manages this.
  </action>
  <verify>
Run: `cd /Users/vaughanfawcett/TradingView && python -m pytest tests/test_performance_tracker.py -v`
All 10 tests pass.
  </verify>
  <done>
PerformanceTracker correctly computes win_rate, profit_factor, and avg_rr for 7d and 30d windows. Upsert logic prevents row duplication. Edge cases (no outcomes, all wins, all losses) handled correctly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire PerformanceTracker into OutcomeDetector and integrate live metrics in StrategySelector</name>
  <files>app/services/outcome_detector.py, app/services/strategy_selector.py</files>
  <action>
**Part A: Wire PerformanceTracker into OutcomeDetector**

Modify `OutcomeDetector.check_outcomes()` to call PerformanceTracker after recording outcomes:

```python
# In OutcomeDetector.__init__, add:
self.performance_tracker = PerformanceTracker()

# At the end of check_outcomes(), after recording all outcomes:
if outcomes:
    # Recalculate metrics only for strategies that had outcomes
    strategy_ids = set()
    for outcome in outcomes:
        signal = await session.get(Signal, outcome.signal_id)
        if signal:
            strategy_ids.add(signal.strategy_id)
    for sid in strategy_ids:
        await self.performance_tracker.recalculate_for_strategy(session, sid)
    await session.commit()
```

This ensures performance recalculation happens exactly once per affected strategy per outcome batch (not every 30 seconds).

**Part B: Integrate live StrategyPerformance in StrategySelector**

Add a method `_fetch_live_metrics` to StrategySelector and modify `select_best` to blend live metrics with backtest scores:

In `select_best()`, after computing backtest-based scores and before applying regime modifier:

```python
# After scores = self._compute_scores(qualified):
# Blend live performance metrics if available
for s in scores:
    live = await self._fetch_live_metrics(session, s.strategy_id)
    if live is not None and live.total_signals >= 5:
        # Blend: 70% backtest score + 30% live performance
        live_composite = self._score_live_metrics(live)
        s.composite_score = 0.7 * s.composite_score + 0.3 * live_composite
        logger.info(
            "Blended live metrics for '{}': live_wr={}, live_pf={}, "
            "blended_score={:.4f}",
            s.strategy_name,
            live.win_rate,
            live.profit_factor,
            s.composite_score,
        )
```

New private method:
```python
async def _fetch_live_metrics(
    self, session: AsyncSession, strategy_id: int
) -> StrategyPerformance | None:
    """Fetch most recent 30d StrategyPerformance row for a strategy."""
    stmt = (
        select(StrategyPerformance)
        .where(
            StrategyPerformance.strategy_id == strategy_id,
            StrategyPerformance.period == "30d",
        )
        .order_by(StrategyPerformance.calculated_at.desc())
        .limit(1)
    )
    result = await session.execute(stmt)
    return result.scalar_one_or_none()

def _score_live_metrics(self, perf: StrategyPerformance) -> float:
    """Convert StrategyPerformance into a [0, 1] composite score.

    Simple: normalize win_rate (already 0-1), profit_factor (cap at 3.0,
    divide by 3.0), avg_rr (cap at 5.0, divide by 5.0).
    Weight: 0.4 * win_rate + 0.35 * pf_norm + 0.25 * rr_norm
    """
```

Add import for StrategyPerformance at the top of strategy_selector.py.

Minimum threshold: only blend if live metrics have >= 5 signals (avoid small sample distortion). Below 5, use pure backtest score.
  </action>
  <verify>
Run: `cd /Users/vaughanfawcett/TradingView && python -m pytest tests/test_performance_tracker.py tests/test_outcome_detector.py -v`
All tests pass.
Run: `cd /Users/vaughanfawcett/TradingView && python -c "from app.services.strategy_selector import StrategySelector; print('OK')"`
Imports cleanly.
  </verify>
  <done>
PerformanceTracker is triggered after each outcome batch. StrategySelector blends live 30d metrics (30% weight) with backtest scores when >= 5 live signals exist. The feedback loop is complete: outcomes -> recalculate metrics -> influence strategy selection.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_performance_tracker.py -v` -- all tests pass
2. `python -m pytest tests/test_outcome_detector.py -v` -- still passes after wiring
3. PerformanceTracker produces correct win_rate, profit_factor, avg_rr for known test data
4. Upsert logic: running recalculate twice does NOT create duplicate rows
5. StrategySelector blends live metrics when available (>= 5 signals) and ignores when not
6. No circular import issues between outcome_detector.py -> performance_tracker.py -> strategy_performance.py
</verification>

<success_criteria>
- PerformanceTracker recalculates 7d and 30d rolling metrics per strategy on demand
- Metrics are upserted (not duplicated) in strategy_performance table
- OutcomeDetector triggers PerformanceTracker after recording outcomes
- StrategySelector blends live 30d metrics into scoring (30% weight when >= 5 signals)
- All tests pass including existing test suites
</success_criteria>

<output>
After completion, create `.planning/phases/06-outcome-tracking-and-feedback/06-02-SUMMARY.md`
</output>
