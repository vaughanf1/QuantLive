---
phase: 04-signal-pipeline
plan: 05
type: execute
wave: 2
depends_on: ["04-01", "04-02", "04-03", "04-04"]
files_modified:
  - app/services/signal_pipeline.py
  - app/workers/jobs.py
  - app/workers/scheduler.py
  - tests/test_signal_pipeline.py
autonomous: true

must_haves:
  truths:
    - "Pipeline orchestrates: select strategy -> generate signals -> validate -> risk check -> enrich -> persist"
    - "Scanner loop runs on H1 candle schedule (hourly at :02) via APScheduler"
    - "Scanner no-ops if no new candle data since last scan"
    - "Pipeline persists approved signals to the signals table with all required fields"
    - "Pipeline expires stale signals before each run"
    - "End-to-end pipeline is testable with mocked services"
  artifacts:
    - path: "app/services/signal_pipeline.py"
      provides: "SignalPipeline orchestrator composing all services"
      exports: ["SignalPipeline"]
      min_lines: 80
    - path: "app/workers/jobs.py"
      provides: "run_signal_scanner() job function"
      contains: "run_signal_scanner"
    - path: "app/workers/scheduler.py"
      provides: "Scanner job registration"
      contains: "run_signal_scanner"
    - path: "tests/test_signal_pipeline.py"
      provides: "Pipeline integration tests"
      min_lines: 50
  key_links:
    - from: "app/services/signal_pipeline.py"
      to: "app/services/strategy_selector.py"
      via: "selector.select_best(session)"
      pattern: "selector\\.select_best"
    - from: "app/services/signal_pipeline.py"
      to: "app/services/signal_generator.py"
      via: "generator.generate(session, strategy_name)"
      pattern: "generator\\.generate"
    - from: "app/services/signal_pipeline.py"
      to: "app/services/risk_manager.py"
      via: "risk_manager.check(session, candidates)"
      pattern: "risk_manager\\.check"
    - from: "app/services/signal_pipeline.py"
      to: "app/services/gold_intelligence.py"
      via: "gold_intel.enrich(candidates)"
      pattern: "gold_intel\\.enrich"
    - from: "app/workers/jobs.py"
      to: "app/services/signal_pipeline.py"
      via: "pipeline.run(session) in run_signal_scanner()"
      pattern: "pipeline\\.run"
    - from: "app/workers/scheduler.py"
      to: "app/workers/jobs.py"
      via: "importing and registering run_signal_scanner"
      pattern: "run_signal_scanner"
---

<objective>
Build the SignalPipeline orchestrator that wires StrategySelector, SignalGenerator, RiskManager, and GoldIntelligence into a sequential flow, then integrate it as an APScheduler job with stale data detection and end-to-end tests.

Purpose: This is the heartbeat of the trading system -- the scheduled job that runs every hour, picks the best strategy, generates signals, validates them, checks risk, enriches with gold intelligence, and persists approved signals. Without this, all the individual services are disconnected.
Output: `app/services/signal_pipeline.py` orchestrator, updated `app/workers/jobs.py` and `scheduler.py`, and `tests/test_signal_pipeline.py` integration tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-signal-pipeline/04-RESEARCH.md
@.planning/phases/04-signal-pipeline/04-01-SUMMARY.md
@.planning/phases/04-signal-pipeline/04-02-SUMMARY.md
@.planning/phases/04-signal-pipeline/04-03-SUMMARY.md
@.planning/phases/04-signal-pipeline/04-04-SUMMARY.md

@app/workers/jobs.py
@app/workers/scheduler.py
@app/models/signal.py
@app/models/candle.py
@app/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SignalPipeline orchestrator and scanner job</name>
  <files>
    app/services/signal_pipeline.py
    app/workers/jobs.py
    app/workers/scheduler.py
  </files>
  <action>
**1. Create `app/services/signal_pipeline.py`:**

```
class SignalPipeline:
    """Orchestrates the full signal generation pipeline.

    Flow: expire stale -> select strategy -> generate candidates ->
          validate (R:R, confidence, dedup, bias) -> risk check ->
          H4 confluence boost -> gold enrichment -> persist.
    """
```

Constructor takes:
- selector: StrategySelector
- generator: SignalGenerator
- risk_manager: RiskManager
- gold_intel: GoldIntelligence

**`async def run(self, session: AsyncSession) -> list[Signal]`:**
1. Expire stale signals: `expired_count = await self.generator.expire_stale_signals(session)`
2. Select best strategy: `best = await self.selector.select_best(session)`. If None, log "No qualifying strategy found, skipping signal generation" and return []
3. Generate candidates: `candidates = await self.generator.generate(session, best.strategy_name)`. If empty, log "No candidates from {strategy_name}" and return []
4. Validate candidates: `validated = await self.generator.validate(session, candidates)`. If empty, log "All candidates filtered out" and return []
5. Risk check: `risk_results = await self.risk_manager.check(session, validated)`. Filter to only approved.
6. For each approved candidate, check H4 confluence: `has_confluence = await self.selector.check_h4_confluence(session, candidate.direction.value)`. If True, boost confidence by +5 (capped at 100) and append " | H4 confluence confirmed" to reasoning.
7. Get DXY correlation (non-blocking): `dxy_info = await self.gold_intel.get_dxy_correlation(session)`
8. Enrich with gold intelligence: `enriched = self.gold_intel.enrich(approved_candidates, dxy_info)`
9. Persist each enriched signal to Signal table:
   - Look up strategy_id from Strategy table (by name)
   - Compute expiry via generator.compute_expiry(candidate)
   - Create Signal ORM object with all fields from CandidateSignal + strategy_id + expires_at + status="active"
   - Convert Decimal fields properly: entry_price, stop_loss, take_profit_1, take_profit_2, risk_reward, confidence
   - Set position_size on each risk_result (store as part of reasoning or as separate metadata -- append to reasoning: f" | Position size: {position_size}")
   - session.add(signal)
10. await session.commit()
11. Log summary: "Pipeline complete: {len(persisted)} signals generated from {strategy_name} (regime={regime})"
12. Return list of persisted Signal objects

**2. Update `app/workers/jobs.py` -- add run_signal_scanner():**

Add a new async function `run_signal_scanner()` following the exact same pattern as `run_daily_backtests()`:
- Top-level try/except to prevent scheduler crashes
- Import strategy modules inside function body (circular import avoidance)
- Create own session via async_session_factory()
- Stale data check FIRST:
  - Query most recent H1 XAUUSD candle timestamp
  - Compare against a stored "last_scan_candle_ts" (use a module-level variable or query for the most recent signal's created_at as proxy)
  - Simpler approach: store `_last_scanned_ts: datetime | None = None` as module-level variable. If the latest candle timestamp == _last_scanned_ts, log "No new candle data, skipping scan" and return. Otherwise update _last_scanned_ts.
- Instantiate all services: StrategySelector(), SignalGenerator(), RiskManager(), GoldIntelligence()
- Create SignalPipeline(selector, generator, risk_manager, gold_intel)
- Call pipeline.run(session)
- Log results

**3. Update `app/workers/scheduler.py`:**

- Add import: `from app.workers.jobs import refresh_candles, run_daily_backtests, run_signal_scanner`
- Register new job in register_jobs():
  ```python
  scheduler.add_job(
      run_signal_scanner,
      trigger=CronTrigger(minute=2, timezone="UTC"),
      id="run_signal_scanner",
      name="Run signal scanner",
      replace_existing=True,
  )
  ```
  - Runs at :02 every hour (1 minute after H1 candle refresh at :01, giving time for candle ingestion)
- Update job count log from 5 to 6

**Important implementation notes:**
- Follow the existing job pattern: imports inside function, try/except wrapper, own session
- The stale data guard prevents duplicate processing (SIG-08) -- if no new H1 candle since last scan, skip
- Signal persistence must map CandidateSignal fields to Signal ORM fields correctly
- strategy_id lookup: query Strategy table WHERE name=strategy_name
- All exceptions in the pipeline should be caught at the job level, not in the pipeline (let errors propagate to the job handler)
- The pipeline itself should NOT catch exceptions (clean separation -- pipeline does logic, job does error handling)
  </action>
  <verify>
Run: `cd /Users/vaughanfawcett/TradingView && python -c "
from app.services.signal_pipeline import SignalPipeline
from app.services.strategy_selector import StrategySelector
from app.services.signal_generator import SignalGenerator
from app.services.risk_manager import RiskManager
from app.services.gold_intelligence import GoldIntelligence
pipeline = SignalPipeline(StrategySelector(), SignalGenerator(), RiskManager(), GoldIntelligence())
print('Pipeline created OK')
from app.workers.jobs import run_signal_scanner
print('Scanner job importable')
"` -- should print "Pipeline created OK" and "Scanner job importable"

Also verify scheduler registration:
`python -c "from app.workers.scheduler import register_jobs; print('Scheduler OK')"` -- should import without error
  </verify>
  <done>
SignalPipeline orchestrator wires all 4 services in correct order. run_signal_scanner() job function exists with stale data guard. Scheduler registers scanner at :02 every hour (6 total jobs).
  </done>
</task>

<task type="auto">
  <name>Task 2: Pipeline integration tests</name>
  <files>tests/test_signal_pipeline.py</files>
  <action>
Create `tests/test_signal_pipeline.py` with unit tests that use mocking to verify pipeline orchestration logic without needing a real database.

**Tests to write (using pytest + unittest.mock):**

1. **test_pipeline_skips_when_no_strategy_qualifies**
   - Mock selector.select_best() to return None
   - Assert pipeline.run() returns empty list
   - Assert generator.generate was NOT called

2. **test_pipeline_skips_when_no_candidates**
   - Mock selector.select_best() to return a StrategyScore
   - Mock generator.generate() to return empty list
   - Assert pipeline.run() returns empty list
   - Assert generator.validate was NOT called

3. **test_pipeline_filters_all_candidates**
   - Mock selector.select_best() to return a StrategyScore
   - Mock generator.generate() to return [mock_candidate]
   - Mock generator.validate() to return empty list (all filtered)
   - Assert pipeline.run() returns empty list

4. **test_pipeline_risk_rejects_all**
   - Mock all services through validation
   - Mock risk_manager.check() to return all rejected results
   - Assert pipeline.run() returns empty list

5. **test_pipeline_full_flow_produces_signal**
   - Mock selector.select_best() -> StrategyScore(strategy_name="liquidity_sweep_reversal", ...)
   - Mock generator.generate() -> [mock_candidate]
   - Mock generator.validate() -> [mock_candidate]
   - Mock risk_manager.check() -> [(mock_candidate, RiskCheckResult(approved=True, position_size=Decimal("1.50")))]
   - Mock selector.check_h4_confluence() -> True
   - Mock gold_intel.get_dxy_correlation() -> DXYCorrelation(available=False, ...)
   - Mock gold_intel.enrich() -> [mock_candidate_enriched]
   - Mock generator.compute_expiry() -> datetime(...)
   - Mock session.execute() for strategy_id lookup -> return mock with scalar_one_or_none() returning 1
   - Verify pipeline returns list with signal(s)
   - Verify session.add was called (signal persisted)
   - Verify session.commit was called

6. **test_stale_data_guard**
   - Test the stale data detection logic from run_signal_scanner
   - This can test the module-level _last_scanned_ts comparison logic

7. **test_expire_stale_signals_called_first**
   - Mock all services
   - Verify generator.expire_stale_signals is called before selector.select_best

Use `unittest.mock.AsyncMock` for all async methods. Use `unittest.mock.MagicMock` for sync methods.

Create helper fixtures for mock candidates:
```python
def make_mock_candidate():
    return CandidateSignal(
        strategy_name="liquidity_sweep_reversal",
        symbol="XAUUSD",
        timeframe="H1",
        direction=Direction.BUY,
        entry_price=Decimal("2650.00"),
        stop_loss=Decimal("2645.00"),
        take_profit_1=Decimal("2660.00"),
        take_profit_2=Decimal("2670.00"),
        risk_reward=Decimal("3.00"),
        confidence=Decimal("75.00"),
        reasoning="Test signal",
        timestamp=datetime.now(timezone.utc),
    )
```
  </action>
  <verify>
Run: `cd /Users/vaughanfawcett/TradingView && PYTHONPATH=. python -m pytest tests/test_signal_pipeline.py -v` -- all tests should pass
  </verify>
  <done>
7 pipeline integration tests pass, covering: no strategy, no candidates, all filtered, risk rejection, full happy path with signal persistence, stale data guard, and expire-before-scan ordering.
  </done>
</task>

</tasks>

<verification>
- `python -c "from app.services.signal_pipeline import SignalPipeline"` imports cleanly
- `python -c "from app.workers.jobs import run_signal_scanner"` imports cleanly
- `PYTHONPATH=. python -m pytest tests/test_signal_pipeline.py -v` -- all 7 tests pass
- Scheduler registers 6 jobs (4 candle + 1 backtest + 1 scanner)
</verification>

<success_criteria>
- SignalPipeline orchestrates all 4 services in correct order
- Pipeline persists approved signals to Signal table with all fields
- Scanner job runs at :02 every hour with stale data guard
- Stale data no-op prevents duplicate processing (SIG-08)
- Expire stale signals runs before each scan cycle
- 7 integration tests verify pipeline logic without database
- Scheduler has 6 registered jobs
</success_criteria>

<output>
After completion, create `.planning/phases/04-signal-pipeline/04-05-SUMMARY.md`
</output>
