---
phase: 03-backtesting-engine
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - app/workers/jobs.py
  - app/workers/scheduler.py
  - tests/test_backtester.py
  - tests/test_trade_simulator.py
  - tests/test_metrics_calculator.py
autonomous: true

must_haves:
  truths:
    - "Daily backtest job runs at 02:00 UTC via APScheduler and backtests all registered strategies on 30-day and 60-day windows"
    - "All backtest results (including walk-forward) are persisted to the backtest_results table with timestamps and parameters"
    - "Test suite validates trade simulation correctness (BUY/SELL, SL priority, EXPIRED), metric calculation, and walk-forward overfitting detection"
    - "Backtest job handles insufficient data gracefully (logs warning, skips strategy, does not crash)"
  artifacts:
    - path: "app/workers/jobs.py"
      provides: "run_daily_backtests() async job function"
      contains: "run_daily_backtests"
    - path: "app/workers/scheduler.py"
      provides: "CronTrigger registration for daily backtest job"
      contains: "run_daily_backtests"
    - path: "tests/test_trade_simulator.py"
      provides: "TradeSimulator unit tests"
      contains: "test_"
    - path: "tests/test_metrics_calculator.py"
      provides: "MetricsCalculator unit tests"
      contains: "test_"
    - path: "tests/test_backtester.py"
      provides: "BacktestRunner and WalkForwardValidator integration tests"
      contains: "test_"
  key_links:
    - from: "app/workers/jobs.py"
      to: "app/services/backtester.py"
      via: "BacktestRunner.run_all_strategies() called inside run_daily_backtests"
      pattern: "BacktestRunner"
    - from: "app/workers/jobs.py"
      to: "app/models/backtest_result.py"
      via: "BacktestResult persistence after each backtest run"
      pattern: "BacktestResult"
    - from: "app/workers/scheduler.py"
      to: "app/workers/jobs.py"
      via: "CronTrigger at 02:00 UTC pointing to run_daily_backtests"
      pattern: "run_daily_backtests"
---

<objective>
Wire the backtesting engine into the APScheduler daily job system, implement result persistence to the database, and create a comprehensive test suite covering trade simulation, metric calculation, and walk-forward validation.

Purpose: This plan makes the backtest engine operational as a scheduled background process and proves its correctness through tests. Without scheduling and persistence, backtests would need manual invocation and results would be lost.

Output: Daily backtest job function, scheduler registration, result persistence, and test suite with 15+ test cases.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-backtesting-engine/03-RESEARCH.md
@.planning/phases/03-backtesting-engine/03-01-SUMMARY.md
@.planning/phases/03-backtesting-engine/03-02-SUMMARY.md

@app/services/backtester.py -- BacktestRunner (from Plan 03-02)
@app/services/walk_forward.py -- WalkForwardValidator (from Plan 03-02)
@app/services/trade_simulator.py -- TradeSimulator, SimulatedTrade, TradeOutcome (from Plan 03-01)
@app/services/spread_model.py -- SessionSpreadModel (from Plan 03-01)
@app/services/metrics_calculator.py -- MetricsCalculator, BacktestMetrics (from Plan 03-01)
@app/models/backtest_result.py -- BacktestResult with walk-forward fields (from Plan 03-02)
@app/models/candle.py -- Candle ORM model
@app/workers/jobs.py -- Existing refresh_candles job function
@app/workers/scheduler.py -- Existing scheduler setup
@app/database.py -- async_session_factory
@app/strategies/base.py -- BaseStrategy.get_registry()
@tests/ -- Existing test patterns from Phase 1
</context>

<tasks>

<task type="auto">
  <name>Task 1: Daily backtest job with result persistence</name>
  <files>app/workers/jobs.py, app/workers/scheduler.py</files>
  <action>
**Update `app/workers/jobs.py`:**

Add `run_daily_backtests()` async function:

```python
async def run_daily_backtests() -> None:
```

Implementation:
1. Import: `BacktestRunner` from `app.services.backtester`, `WalkForwardValidator` from `app.services.walk_forward`, `BacktestResult` from `app.models.backtest_result`, `Candle` from `app.models.candle`, `BaseStrategy` from `app.strategies.base`, `candles_to_dataframe` from `app.strategies.base`, `select` from `sqlalchemy`
2. Also import all concrete strategies to trigger registration:
   ```python
   import app.strategies.liquidity_sweep  # noqa: F401
   import app.strategies.trend_continuation  # noqa: F401
   import app.strategies.breakout_expansion  # noqa: F401
   ```
3. Wrap everything in try/except (same pattern as refresh_candles) to prevent scheduler crashes
4. Create `BacktestRunner()` and `WalkForwardValidator(runner=runner)` instances
5. Open async session via `async_session_factory()`
6. Query candles: Fetch all H1 XAUUSD candles ordered by timestamp ascending. Use `select(Candle).where(Candle.symbol == "XAUUSD", Candle.timeframe == "H1").order_by(Candle.timestamp.asc())`. Convert to DataFrame via `candles_to_dataframe(candles_list)`.
7. If fewer than `30 * 24 + 72` candles (minimum for 30-day window + MAX_BARS_FORWARD), log warning and return
8. For each registered strategy (from `BaseStrategy.get_registry()`):
   a. For each window_days in [30, 60]:
      - Run `runner.run_full_backtest(strategy_instance, candles_df, window_days)` to get (metrics, trades)
      - If total_trades == 0: log info and skip persistence for this window
      - Query `strategy_id` from strategies table by name (or log warning if not found and skip)
      - Create BacktestResult ORM object with:
        - strategy_id, timeframe="H1", window_days, start_date (first candle timestamp), end_date (last candle timestamp)
        - All 5 metrics from BacktestMetrics
        - is_walk_forward=False, spread_model="session_aware"
      - `session.add(result)` and `await session.commit()`
   b. Run walk-forward validation:
      - `wf_result = wfv.validate(strategy_instance, candles_df, window_days=30)`
      - If wf_result.oos_metrics.total_trades > 0:
        - Create BacktestResult with OOS metrics, is_walk_forward=True, is_overfitted=wf_result.is_overfitted, walk_forward_efficiency=Decimal(str(round(wf_result.wfe_win_rate, 4))) if not None else None
        - Persist to DB
      - Log walk-forward result: strategy name, is_overfitted, WFE ratios
9. Log summary: total strategies processed, total results persisted

**Update `app/workers/scheduler.py`:**

1. Add import: `from app.workers.jobs import run_daily_backtests` (alongside existing refresh_candles import)
2. In `register_jobs()`, add after existing candle refresh jobs:
   ```python
   scheduler.add_job(
       run_daily_backtests,
       trigger=CronTrigger(hour=2, minute=0, timezone="UTC"),
       id="run_daily_backtests",
       name="Run daily strategy backtests",
       replace_existing=True,
   )
   logger.info("Registered job: run_daily_backtests (daily at 02:00 UTC)")
   ```
3. Update the final log count from 4 to 5

**Critical constraints:**
- Job creates its own session via async_session_factory (not FastAPI dependency injection) -- same pattern as refresh_candles
- All exceptions caught at top level to prevent scheduler death
- Strategy imports done inside function to avoid circular imports
- Log with loguru at info level for normal operations, warning for insufficient data, exception for errors
  </action>
  <verify>
```bash
cd /Users/vaughanfawcett/TradingView && python -c "
from app.workers.jobs import run_daily_backtests
from app.workers.scheduler import register_jobs
import inspect

# Verify function exists and is async
assert inspect.iscoroutinefunction(run_daily_backtests)
print('run_daily_backtests: exists and is async')

# Verify scheduler has backtest job after registration
# (can't actually start scheduler in test, just verify code parses)
print('ALL CHECKS PASSED')
"
```
  </verify>
  <done>run_daily_backtests() fetches H1 candles, runs all strategies on 30/60-day windows, runs walk-forward validation, and persists all results to backtest_results table. Scheduled at 02:00 UTC daily.</done>
</task>

<task type="auto">
  <name>Task 2: Test suite for backtesting engine</name>
  <files>tests/test_trade_simulator.py, tests/test_metrics_calculator.py, tests/test_backtester.py</files>
  <action>
Create three test files using pytest. All tests use synthetic candle data (no database needed for unit tests). Use `from decimal import Decimal`, `import pandas as pd`, `from datetime import datetime, timezone`.

**File 1: `tests/test_trade_simulator.py`**

Test cases for TradeSimulator:
1. `test_buy_tp1_hit` -- BUY signal where TP1 is reached: verify outcome=TP1_HIT, positive pnl_pips, correct bars_held
2. `test_buy_tp2_hit` -- BUY signal where price reaches TP2: verify outcome=TP2_HIT
3. `test_buy_sl_hit` -- BUY signal where SL is hit: verify outcome=SL_HIT, negative pnl_pips
4. `test_sell_tp1_hit` -- SELL signal where price drops to TP1: verify outcome=TP1_HIT, positive pnl_pips
5. `test_sell_sl_hit` -- SELL signal where price rises to SL: verify outcome=SL_HIT, negative pnl_pips
6. `test_sl_priority_over_tp` -- Bar where both SL and TP could be hit (bar_low < SL AND bar_high > TP1): verify SL wins (conservative)
7. `test_expired_no_hit` -- Only provide MAX_BARS_FORWARD bars that don't hit any level: verify outcome=EXPIRED
8. `test_spread_adjusts_buy_entry` -- BUY with spread: effective entry is higher, verify pnl reflects spread cost
9. `test_no_lookahead` -- Signal at bar 0, verify bar 0 is NOT checked for SL/TP (only bar 1+)

Create a pytest fixture `make_signal` that creates CandidateSignal with sensible defaults and accepts overrides for direction, entry, SL, TP1, TP2.

Create a helper function `make_candles(data: list[dict])` that builds a DataFrame from a list of {open, high, low, close} dicts with auto-generated timestamps.

**File 2: `tests/test_metrics_calculator.py`**

Test cases for MetricsCalculator:
1. `test_empty_trades` -- Empty list returns zeroed metrics with total_trades=0
2. `test_all_winners` -- 3 winning trades: win_rate=1.0, profit_factor=9999.9999 (capped), positive expectancy
3. `test_all_losers` -- 3 losing trades: win_rate=0.0, profit_factor=0, negative expectancy
4. `test_mixed_trades` -- 3 wins + 1 loss: correct win_rate (0.75), profit_factor > 1, positive expectancy
5. `test_single_trade` -- 1 trade: sharpe_ratio=0 (can't compute std of 1), correct win_rate
6. `test_max_drawdown` -- Sequence with known drawdown: wins then losses, verify max_drawdown matches expected

For tests needing SimulatedTrade objects, create a `make_simulated_trade(pnl_pips: float)` fixture/helper that returns a SimulatedTrade with the given pnl_pips.

**File 3: `tests/test_backtester.py`**

Test cases for BacktestRunner and WalkForwardValidator:
1. `test_runner_instantiation` -- BacktestRunner creates with default components
2. `test_rolling_backtest_insufficient_data` -- Candles fewer than window requirement: returns empty trades, no crash
3. `test_rolling_backtest_uses_analyze` -- Mock a strategy's analyze() method, verify it's called with windowed candles (proving BACK-02: same code path)
4. `test_walk_forward_insufficient_oos_trades` -- WalkForwardValidator with very small dataset: verify insufficient_oos_trades=True, is_overfitted=False
5. `test_walk_forward_detects_overfitting` -- Mock runner to return high IS metrics and low OOS metrics: verify is_overfitted=True

For BacktestRunner tests that need strategy mocking, use `unittest.mock.MagicMock` to create a fake strategy with a controllable `analyze()` return value.

**Test helpers pattern:**
```python
import pytest
from decimal import Decimal
from datetime import datetime, timezone, timedelta
import pandas as pd

def make_candles(n: int, base_price: float = 2650.0, volatility: float = 5.0) -> pd.DataFrame:
    """Generate n synthetic H1 candles with realistic OHLC."""
    ...
```

**Critical constraints:**
- All tests are pure unit tests (no database, no async) for simulator and metrics
- BacktestRunner tests may use mocks for strategy.analyze()
- Tests must run with: `cd /Users/vaughanfawcett/TradingView && python -m pytest tests/test_trade_simulator.py tests/test_metrics_calculator.py tests/test_backtester.py -v`
- Use `Decimal` assertions where outputs are Decimal
  </action>
  <verify>
```bash
cd /Users/vaughanfawcett/TradingView && python -m pytest tests/test_trade_simulator.py tests/test_metrics_calculator.py tests/test_backtester.py -v --tb=short 2>&1 | tail -30
```
  </verify>
  <done>All 15+ tests pass. Trade simulator correctness verified for BUY/SELL/SL-priority/EXPIRED. Metrics calculator handles edge cases. BacktestRunner confirms same analyze() code path (BACK-02). Walk-forward detects overfitting correctly.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_trade_simulator.py tests/test_metrics_calculator.py tests/test_backtester.py -v` -- all tests pass
2. `python -c "from app.workers.jobs import run_daily_backtests; import inspect; assert inspect.iscoroutinefunction(run_daily_backtests)"` -- job function is async
3. Scheduler registers 5 jobs (4 candle refresh + 1 daily backtest) -- verify in register_jobs output
4. BacktestResult persistence includes walk-forward fields (is_walk_forward, is_overfitted, walk_forward_efficiency, spread_model)
5. Job handles insufficient candle data without crashing
</verification>

<success_criteria>
- run_daily_backtests() is registered as APScheduler CronTrigger at 02:00 UTC
- Job queries H1 XAUUSD candles, runs all strategies on 30-day and 60-day windows, persists BacktestResult rows
- Walk-forward results are persisted with is_walk_forward=True and overfitting flag
- Test suite has 15+ test cases covering: trade simulation (BUY/SELL/SL-priority/EXPIRED/spread), metric calculation (empty/all-win/all-loss/mixed/single/drawdown), and runner integration (insufficient data, same analyze() path, overfitting detection)
- All tests pass on `python -m pytest tests/ -v`
</success_criteria>

<output>
After completion, create `.planning/phases/03-backtesting-engine/03-03-SUMMARY.md`
</output>
